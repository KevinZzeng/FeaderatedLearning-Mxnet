# Privacy-Preserving Deep Learning

## Distributed Selective SGD

Observations：

1 参数更新各自独立

2 不同训练集对不同的参数贡献不同

3 不同的特征对目标函数贡献不同

Algorithms：

​	选择参数更新 Selective SGD：

​		算法参数 θ：参数选择比例

​		将参数占比为θ，取梯度值最大的一部分用于更新模型

​	分布式协同训练 Distributed collaborative learning：

​		参与者之间交流对部分参数的梯度

​		参与者控制梯度信息交流的参数与频率

​	参与者端训练算法：

​		本地网络随机初始化？

​		1 从服务器下载神经网络参数分片，覆盖本地参数值

​		2 用本地数据集训练当前网络 1 epoch

​		3 计算梯度Δ

​		4 选择梯度上传  

​	服务器端训练算法：

​		1 初始化

​		2 梯度更新

​		3 记录更新时间 当有参与者询问参数时  将最近更新的参数传出



参数交换协议：

round robin：循环

random order：随机顺序

asychronous：异步