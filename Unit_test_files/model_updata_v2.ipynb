{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit000112c82aae46049a43b9e45b233c07",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch SGD训练下\n",
    "# 梯度传输\n",
    "\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import autograd as ag\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import loss \n",
    "import copy\n",
    "\n",
    "def LeNet_(activation='relu'):\n",
    "    # 获取一个结构定义完整的LeNet卷积神经网络\n",
    "    # 激活函数可自选 默认为sigmoid\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Conv2D(channels=6, kernel_size=(5,5), activation=activation),\n",
    "            nn.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "            nn.Conv2D(channels=16, kernel_size=(5,5), activation=activation),\n",
    "            nn.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "            # Dense会默认将(批量大小， 通道， 高， 宽)形状的输入转换成\n",
    "            # (批量大小， 通道 * 高 * 宽)形状的输入\n",
    "            nn.Dense(120, activation=activation),\n",
    "            nn.Dense(84, activation=activation),\n",
    "            nn.Dense(10))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "net = LeNet_()\n",
    "input_shape = (1,1,28,28)\n",
    "ctx = [mx.gpu()]\n",
    "mx.random.seed(42)\n",
    "net.initialize(mx.init.Xavier(magnitude=2.24),ctx=ctx)\n",
    "_ = net(nd.random.uniform(shape=input_shape,ctx=ctx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "初始模型验证准确率 validation acc:accuracy=0.100000\n"
    }
   ],
   "source": [
    "#验证\n",
    "mnist = mx.test_utils.get_mnist()\n",
    "val_data = mx.io.NDArrayIter(mnist['test_data'],mnist['test_label'],batch_size=100)    \n",
    "for batch in val_data:\n",
    "    data = gluon.utils.split_and_load(batch.data[0],ctx_list=ctx,batch_axis=0)\n",
    "    label = gluon.utils.split_and_load(batch.label[0],ctx_list=ctx,batch_axis=0)\n",
    "    outputs = []\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for x in data:\n",
    "        outputs.append(net(x))\n",
    "    metric.update(label,outputs)\n",
    "print('初始模型验证准确率 validation acc:%s=%f'%metric.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_dict():\n",
    "    grad_dict = {}\n",
    "    grad_dict['weight'] = []\n",
    "    grad_dict['bias'] = []\n",
    "    \"\"\"\n",
    "    for layer in net:\n",
    "        try:\n",
    "            shape_w = layer.weight.data().shape\n",
    "            shape_b = layer.bias.data().shape\n",
    "        except:\n",
    "            continue\n",
    "        grad_dict['weight'].append(nd.zeros(shape=shape_w,ctx=ctx[0]))\n",
    "        grad_dict['bias'].append(nd.zeros(shape=shape_b,ctx=ctx[0]))\n",
    "    \"\"\"\n",
    "    return grad_dict\n",
    "\n",
    "def collect_gradient(net,grad_dict_list,batch_size):\n",
    "    #idx = 0\n",
    "    grad_dic = grad_dict()\n",
    "    for layer in net:\n",
    "        try:\n",
    "            grad_w = layer.weight.data().grad\n",
    "            grad_b = layer.bias.data().grad\n",
    "        except:\n",
    "            continue\n",
    "        grad_dic[\"weight\"].append(grad_w/batch_size)\n",
    "        grad_dic[\"bias\"].append(grad_b/batch_size)\n",
    "        #idx+=1\n",
    "    grad_dict_list.append(grad_dic)\n",
    "\n",
    "def updata_gradient(net,grad_dict_list,learning_rate):\n",
    "    # 由Client回传的梯度信息 更新Server模型\n",
    "    idx = 0\n",
    "    for grad_dic in grad_dict_list:\n",
    "        grad_w = grad_dict['weight']\n",
    "        grad_b = grad_dict['bias']\n",
    "        update_flag = False\n",
    "        for layer in net:\n",
    "            try:\n",
    "                layer.weight.data()[:] = layer.weight.data()[:] - learning_rate*grad_w[idx]\n",
    "                #layer.weight.set_data(layer.weight.data()[:] - learning_rate*gradient_info[idx])\n",
    "                layer.bias.data()[:] = layer.bias.data()[:] - learning_rate*grad_b[idx]\n",
    "            except:\n",
    "                continue\n",
    "            idx += 1\n",
    "    \"\"\"\n",
    "    if update_flag:\n",
    "        print(\"-gradient successfully updated-\")\n",
    "    else:\n",
    "        print(\"-gradient failure-\")\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "train_data = mx.io.NDArrayIter(mnist['train_data'],mnist['train_label'],batch_size=100) \n",
    "epoch = 10\n",
    "metric = mx.metric.Accuracy()\n",
    "smc_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnboundLocalError",
     "evalue": "local variable 'grad_dict' referenced before assignment",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6312ec61df2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#收集梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mcollect_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_dict_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m#updata_gradient(origin_net,gradient_info,learning_rate=0.02)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b18cb4868ac7>\u001b[0m in \u001b[0;36mcollect_gradient\u001b[1;34m(net, grad_dict_list, batch_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcollect_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrad_dict_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#idx = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mgrad_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'grad_dict' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# 正常训练\n",
    "origin_net = copy.deepcopy(net)\n",
    "grad_dict_list = []\n",
    "for i in range(epoch):\n",
    "    train_data.reset()\n",
    "    for batch in train_data:\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        with ag.record():\n",
    "            for x,y in zip(data,label):\n",
    "                z = net(x)\n",
    "                loss = smc_loss(z, y)\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "        #收集梯度\n",
    "        collect_gradient(net,grad_dict_list,batch_size=batch.data[0].shape[0])\n",
    "        #updata_gradient(origin_net,gradient_info,learning_rate=0.02)\n",
    "        metric.update(label,outputs)\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "    name,acc = metric.get()\n",
    "    metric.reset()\n",
    "    print('training acc at epoch %d, %s=%f'%(i,name,acc))\n",
    "    # YA DA ZE\n",
    "updata_gradient(origin_net, grad_dict_list, learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}